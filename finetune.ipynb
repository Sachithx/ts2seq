{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0315c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check cuda works pytorch\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EXTRACT ENCODER FROM PIX2SEQ CHECKPOINT\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: LOAD PIX2SEQ CHECKPOINT\n",
    "# ============================================================================\n",
    "\n",
    "MODEL_DIR = \"/home/AD/sachith/pix2seq/data/HAR_pretrained/ts_model\"\n",
    "\n",
    "# Find the latest checkpoint\n",
    "checkpoint_path = tf. train.latest_checkpoint(MODEL_DIR)\n",
    "print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = tf.train.load_checkpoint(checkpoint_path)\n",
    "\n",
    "# Inspect variables in checkpoint\n",
    "print(\"\\n=== Checkpoint Variables ===\")\n",
    "var_shapes = checkpoint. get_variable_to_shape_map()\n",
    "for var_name in sorted(var_shapes.keys())[:20]:  # Show first 20\n",
    "    print(f\"{var_name}: {var_shapes[var_name]}\")\n",
    "\n",
    "print(f\"\\n...  ({len(var_shapes)} total variables)\")\n",
    "\n",
    "# Filter encoder variables\n",
    "encoder_vars = {k: v for k, v in var_shapes.items() \n",
    "                if 'encoder' in k.lower() or 'backbone' in k.lower() or 'image' in k.lower()}\n",
    "\n",
    "print(f\"\\n=== Encoder Variables ({len(encoder_vars)}) ===\")\n",
    "for var_name in sorted(encoder_vars. keys())[:20]:\n",
    "    print(f\"{var_name}: {encoder_vars[var_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b2ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 2: IDENTIFY ENCODER ARCHITECTURE\n",
    "# ============================================================================\n",
    "\n",
    "def identify_encoder_type(var_names):\n",
    "    \"\"\"Identify what type of encoder is used.\"\"\"\n",
    "    var_names_str = ' '.join(var_names).lower()\n",
    "    \n",
    "    if 'vit' in var_names_str or 'vision_transformer' in var_names_str: \n",
    "        return 'ViT'\n",
    "    elif 'resnet' in var_names_str: \n",
    "        return 'ResNet'\n",
    "    elif 'efficientnet' in var_names_str:\n",
    "        return 'EfficientNet'\n",
    "    elif 'convnet' in var_names_str or 'conv' in var_names_str: \n",
    "        return 'ConvNet'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "encoder_type = identify_encoder_type(list(var_shapes.keys()))\n",
    "print(f\"\\n=== Detected Encoder Type:  {encoder_type} ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f8d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 3: EXTRACT ENCODER WEIGHTS\n",
    "# ============================================================================\n",
    "\n",
    "def extract_encoder_weights(checkpoint, encoder_type='ViT'):\n",
    "    \"\"\"Extract encoder weights from checkpoint.\"\"\"\n",
    "    \n",
    "    weights = {}\n",
    "    \n",
    "    # Get all variable names\n",
    "    var_names = checkpoint.get_variable_to_shape_map().keys()\n",
    "    \n",
    "    # Filter encoder variables (adjust patterns based on your model)\n",
    "    encoder_patterns = [\n",
    "        'encoder',\n",
    "        'backbone',\n",
    "        'image_encoder',\n",
    "        'visual_encoder',\n",
    "        'feature_extractor',\n",
    "        # Add patterns specific to your architecture\n",
    "    ]\n",
    "    \n",
    "    for var_name in var_names: \n",
    "        # Check if this is an encoder variable\n",
    "        is_encoder_var = any(pattern in var_name. lower() for pattern in encoder_patterns)\n",
    "        \n",
    "        # Exclude decoder variables\n",
    "        is_decoder_var = any(pattern in var_name.lower() \n",
    "                            for pattern in ['decoder', 'output_head', 'token_embed'])\n",
    "        \n",
    "        if is_encoder_var and not is_decoder_var:\n",
    "            try:\n",
    "                tensor = checkpoint.get_tensor(var_name)\n",
    "                weights[var_name] = tensor\n",
    "                print(f\"Extracted:  {var_name} {tensor.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to extract {var_name}: {e}\")\n",
    "    \n",
    "    return weights\n",
    "\n",
    "encoder_weights = extract_encoder_weights(checkpoint, encoder_type)\n",
    "\n",
    "print(f\"\\n=== Extracted {len(encoder_weights)} encoder weight tensors ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b16739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 4: SAVE ENCODER WEIGHTS\n",
    "# ============================================================================\n",
    "\n",
    "SAVE_DIR = \"/home/AD/sachith/pix2seq/data/HAR_pretrained/extracted_encoder\"\n",
    "Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as numpy archive\n",
    "np.savez(f\"{SAVE_DIR}/encoder_weights.npz\", **encoder_weights)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'encoder_type': encoder_type,\n",
    "    'checkpoint_path': checkpoint_path,\n",
    "    'num_weights': len(encoder_weights),\n",
    "    'variable_names': list(encoder_weights.keys()),\n",
    "    'shapes': {k: list(v.shape) for k, v in encoder_weights.items()}\n",
    "}\n",
    "\n",
    "with open(f\"{SAVE_DIR}/encoder_metadata.json\", 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Saved encoder weights to {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba9f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# COMPLETE CONVERSION SCRIPT - CLEAN VERSION\n",
    "# ============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Paths\n",
    "MODEL_DIR = \"/home/AD/sachith/pix2seq/data/HAR_pretrained/ts_model\"\n",
    "SAVE_DIR = \"/home/AD/sachith/pix2seq/data/HAR_pretrained/extracted_encoder\"\n",
    "Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PIX2SEQ ENCODER EXTRACTION & CONVERSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load TensorFlow checkpoint\n",
    "checkpoint_path = tf.train.latest_checkpoint(MODEL_DIR)\n",
    "print(f\"\\n1. Loading TensorFlow checkpoint: {checkpoint_path}\")\n",
    "checkpoint = tf.train.load_checkpoint(checkpoint_path)\n",
    "\n",
    "# Extract encoder variables\n",
    "var_shapes = checkpoint.get_variable_to_shape_map()\n",
    "encoder_vars = {k: v for k, v in var_shapes.items() \n",
    "                if 'encoder' in k.lower() and 'decoder' not in k.lower()}\n",
    "\n",
    "print(f\"   Found {len(encoder_vars)} encoder variables\")\n",
    "\n",
    "# Extract weights\n",
    "encoder_weights = {}\n",
    "for var_name in encoder_vars.keys():\n",
    "    tensor = checkpoint.get_tensor(var_name)\n",
    "    encoder_weights[var_name] = tensor\n",
    "\n",
    "print(f\"   Extracted {len(encoder_weights)} weight tensors\")\n",
    "\n",
    "# %%\n",
    "# ============================================================================\n",
    "# CONVERT TO PYTORCH FORMAT\n",
    "# ============================================================================\n",
    "\n",
    "def convert_to_pytorch(tf_weights, hidden_dim=768, num_heads=12):\n",
    "    \"\"\"Convert TensorFlow Pix2Seq encoder weights to PyTorch format.\"\"\"\n",
    "    \n",
    "    pt_weights = {}\n",
    "    converted_count = 0\n",
    "    \n",
    "    for tf_name, tf_tensor in tf_weights.items():\n",
    "        pt_tensor = None\n",
    "        pt_name = None\n",
    "        \n",
    "        # STEM CONV: [H, W, C_in, C_out] -> [C_out, C_in, H, W]\n",
    "        if 'stem_conv/kernel' in tf_name:\n",
    "            pt_tensor = np.transpose(tf_tensor, (3, 2, 0, 1))\n",
    "            pt_name = 'stem_conv.weight'\n",
    "        elif 'stem_conv/bias' in tf_name:\n",
    "            pt_tensor = tf_tensor\n",
    "            pt_name = 'stem_conv.bias'\n",
    "        \n",
    "        # STEM LAYER NORM\n",
    "        elif 'stem_ln/gamma' in tf_name:\n",
    "            pt_tensor = tf_tensor\n",
    "            pt_name = 'stem_ln.weight'\n",
    "        elif 'stem_ln/beta' in tf_name:\n",
    "            pt_tensor = tf_tensor\n",
    "            pt_name = 'stem_ln.bias'\n",
    "        \n",
    "        # OUTPUT LAYER NORM\n",
    "        elif 'output_ln/gamma' in tf_name:\n",
    "            pt_tensor = tf_tensor\n",
    "            pt_name = 'output_ln.weight'\n",
    "        elif 'output_ln/beta' in tf_name:\n",
    "            pt_tensor = tf_tensor\n",
    "            pt_name = 'output_ln.bias'\n",
    "        \n",
    "        # ENCODER LAYERS\n",
    "        elif 'enc_layers/' in tf_name:\n",
    "            import re\n",
    "            layer_match = re.search(r'enc_layers/(\\d+)/', tf_name)\n",
    "            if not layer_match:\n",
    "                continue\n",
    "            \n",
    "            layer_idx = int(layer_match.group(1))\n",
    "            \n",
    "            # ATTENTION LAYERS\n",
    "            if '/mha/' in tf_name:\n",
    "                # Q/K/V: [hidden, num_heads, head_dim] -> [hidden, hidden]\n",
    "                if '_query_dense/kernel' in tf_name:\n",
    "                    pt_tensor = tf_tensor.reshape(hidden_dim, -1).T if len(tf_tensor.shape) == 3 else tf_tensor.T\n",
    "                    pt_name = f'encoder_layers.{layer_idx}.mha.query_dense.weight'\n",
    "                elif '_query_dense/bias' in tf_name:\n",
    "                    pt_tensor = tf_tensor.flatten() if len(tf_tensor.shape) > 1 else tf_tensor\n",
    "                    pt_name = f'encoder_layers.{layer_idx}.mha.query_dense.bias'\n",
    "                elif '_key_dense/kernel' in tf_name:\n",
    "                    pt_tensor = tf_tensor.reshape(hidden_dim, -1).T if len(tf_tensor.shape) == 3 else tf_tensor.T\n",
    "                    pt_name = f'encoder_layers.{layer_idx}.mha.key_dense.weight'\n",
    "                elif '_key_dense/bias' in tf_name:\n",
    "                    pt_tensor = tf_tensor.flatten() if len(tf_tensor.shape) > 1 else tf_tensor\n",
    "                    pt_name = f'encoder_layers.{layer_idx}.mha.key_dense.bias'\n",
    "                elif '_value_dense/kernel' in tf_name:\n",
    "                    pt_tensor = tf_tensor.reshape(hidden_dim, -1).T if len(tf_tensor.shape) == 3 else tf_tensor.T\n",
    "                    pt_name = f'encoder_layers.{layer_idx}.mha.value_dense.weight'\n",
    "                elif '_value_dense/bias' in tf_name:\n",
    "                    pt_tensor = tf_tensor.flatten() if len(tf_tensor.shape) > 1 else tf_tensor\n",
    "                    pt_name = f'encoder_layers.{layer_idx}.mha.value_dense.bias'\n",
    "                # Output: [num_heads, head_dim, hidden] -> [hidden, hidden]\n",
    "                elif '_output_dense/kernel' in tf_name:\n",
    "                    pt_tensor = tf_tensor.reshape(-1, hidden_dim).T if len(tf_tensor.shape) == 3 else tf_tensor.T\n",
    "                    pt_name = f'encoder_layers.{layer_idx}.mha.output_dense.weight'\n",
    "                elif '_output_dense/bias' in tf_name:\n",
    "                    pt_tensor = tf_tensor\n",
    "                    pt_name = f'encoder_layers.{layer_idx}.mha.output_dense.bias'\n",
    "            \n",
    "            # ATTENTION LAYER NORM\n",
    "            elif '/mha_ln/gamma' in tf_name:\n",
    "                pt_tensor = tf_tensor\n",
    "                pt_name = f'encoder_layers.{layer_idx}.mha_ln.weight'\n",
    "            elif '/mha_ln/beta' in tf_name:\n",
    "                pt_tensor = tf_tensor\n",
    "                pt_name = f'encoder_layers.{layer_idx}.mha_ln.bias'\n",
    "            \n",
    "            # MLP LAYERS\n",
    "            elif '/mlp/' in tf_name:\n",
    "                if 'dense1/kernel' in tf_name:\n",
    "                    pt_tensor = tf_tensor.T\n",
    "                    pt_name = f'encoder_layers.{layer_idx}.mlp.dense1.weight'\n",
    "                elif 'dense1/bias' in tf_name:\n",
    "                    pt_tensor = tf_tensor\n",
    "                    pt_name = f'encoder_layers.{layer_idx}.mlp.dense1.bias'\n",
    "                elif 'dense2/kernel' in tf_name:\n",
    "                    pt_tensor = tf_tensor.T\n",
    "                    pt_name = f'encoder_layers.{layer_idx}.mlp.dense2.weight'\n",
    "                elif 'dense2/bias' in tf_name:\n",
    "                    pt_tensor = tf_tensor\n",
    "                    pt_name = f'encoder_layers.{layer_idx}.mlp.dense2.bias'\n",
    "                # MLP Layer Norm (nested inside mlp module)\n",
    "                elif 'layernorms/0/gamma' in tf_name:\n",
    "                    pt_tensor = tf_tensor\n",
    "                    pt_name = f'encoder_layers.{layer_idx}.mlp.layernorm.weight'\n",
    "                elif 'layernorms/0/beta' in tf_name:\n",
    "                    pt_tensor = tf_tensor\n",
    "                    pt_name = f'encoder_layers.{layer_idx}.mlp.layernorm.bias'\n",
    "        \n",
    "        if pt_tensor is not None and pt_name is not None:\n",
    "            pt_weights[pt_name] = torch.from_numpy(pt_tensor)\n",
    "            converted_count += 1\n",
    "    \n",
    "    return pt_weights, converted_count\n",
    "\n",
    "print(\"\\n2. Converting to PyTorch format...\")\n",
    "pt_weights, converted_count = convert_to_pytorch(encoder_weights)\n",
    "print(f\"   Converted {converted_count} parameters\")\n",
    "\n",
    "# Save PyTorch weights\n",
    "torch_path = f\"{SAVE_DIR}/encoder_weights.pth\"\n",
    "torch.save(pt_weights, torch_path)\n",
    "print(f\"\\n3. Saved PyTorch weights: {torch_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'source': checkpoint_path,\n",
    "    'architecture': 'ViT-Base',\n",
    "    'hidden_dim': 768,\n",
    "    'num_layers': 12,\n",
    "    'num_heads': 12,\n",
    "    'num_parameters': converted_count,\n",
    "    'parameter_names': list(pt_weights.keys())\n",
    "}\n",
    "\n",
    "with open(f\"{SAVE_DIR}/metadata.json\", 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"   Saved metadata: {SAVE_DIR}/metadata.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc68ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.encoder_cls import EncoderClassifier\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create model\n",
    "    model = EncoderClassifier(num_classes=6, pretrained_encoder_path='/home/AD/sachith/pix2seq/data/HAR_pretrained/extracted_encoder/encoder_weights.pth', freeze_encoder=True, hidden_dims=[512], dropout=0.1, image_size=224)\n",
    "    \n",
    "    # Test\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Dummy input\n",
    "    x = torch.randn(2, 3, 224, 224).to(device)\n",
    "    logits = model(x)\n",
    "    \n",
    "    print(f\"\\nTest output shape: {logits.shape}\")  # [2, 6]\n",
    "    print(f\"✓ Model ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a0ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# PART 7: TEST LOADING WITH PYTORCH WEIGHTS\n",
    "# ============================================================================\n",
    "\n",
    "from models.encoder_cls import EncoderClassifier\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING WEIGHT LOADING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model = EncoderClassifier(\n",
    "    num_classes=6,\n",
    "    pretrained_encoder_path=f'{SAVE_DIR}/encoder_weights.pth',\n",
    "    freeze_encoder=True,\n",
    "    hidden_dims=[512],\n",
    "    dropout=0.1,\n",
    "    image_size=224\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Model created successfully!\")\n",
    "\n",
    "# Test forward pass\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "x = torch.randn(2, 3, 224, 224).to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(x)\n",
    "\n",
    "print(f\"\\nTest output shape: {logits.shape}\")  # [2, 6]\n",
    "print(f\"✓ Forward pass successful!\")\n",
    "\n",
    "# %%\n",
    "# Verify which parameters are trainable\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINABLE PARAMETERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Trainable: {trainable:,}\")\n",
    "print(f\"Total: {total:,}\")\n",
    "print(f\"Frozen: {total - trainable:,}\")\n",
    "\n",
    "print(\"\\nTrainable layers:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"  {name}: {tuple(param.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a2511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# VERIFY CONVERSION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from models.encoder_cls import EncoderClassifier\n",
    "\n",
    "# Load model\n",
    "model = EncoderClassifier(\n",
    "    num_classes=6,\n",
    "    pretrained_encoder_path=torch_path,\n",
    "    freeze_encoder=True,\n",
    "    hidden_dims=[512],\n",
    "    dropout=0.1,\n",
    "    image_size=224\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "x = torch.randn(2, 3, 224, 224).to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(x)\n",
    "\n",
    "print(f\"\\n✓ Forward pass successful!\")\n",
    "print(f\"  Input shape:  {tuple(x.shape)}\")\n",
    "print(f\"  Output shape: {tuple(logits.shape)}\")\n",
    "\n",
    "# Parameter counts\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "frozen = total - trainable\n",
    "\n",
    "print(f\"\\n  Total parameters:     {total:,}\")\n",
    "print(f\"  Frozen (encoder):     {frozen:,}\")\n",
    "print(f\"  Trainable (head):     {trainable:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ CONVERSION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nPretrained encoder ready at:\")\n",
    "print(f\"  {torch_path}\")\n",
    "print(f\"\\nYou can now fine-tune on your HAR dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# QUICK TEST\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUICK TEST - VERIFYING EVERYTHING WORKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from models.encoder_cls import EncoderClassifier\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "model = EncoderClassifier(\n",
    "    num_classes=6,\n",
    "    pretrained_encoder_path='/home/AD/sachith/pix2seq/data/HAR_pretrained/extracted_encoder/encoder_weights.pth',\n",
    "    freeze_encoder=True,\n",
    "    hidden_dims=[512],\n",
    "    dropout=0.1,\n",
    "    image_size=224\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Test batch\n",
    "batch_size = 8\n",
    "x = torch.randn(batch_size, 3, 224, 224).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(x)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "print(f\"\\n✓ Test passed!\")\n",
    "print(f\"  Input:  {tuple(x.shape)}\")\n",
    "print(f\"  Logits: {tuple(logits.shape)}\")\n",
    "print(f\"  Probs:  {tuple(probs.shape)}\")\n",
    "print(f\"  Prob sum per sample: {probs.sum(dim=1)}\")  # Should be ~1.0\n",
    "\n",
    "# Check gradients\n",
    "print(f\"\\n✓ Gradient flow check:\")\n",
    "print(f\"  Encoder frozen: {not any(p.requires_grad for p in model.encoder.parameters())}\")\n",
    "print(f\"  Head trainable: {all(p.requires_grad for p in model.classifier.parameters())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL CHECKS PASSED - READY FOR TRAINING!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455ee584",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert Multivariate HAR to Multi-Channel Images\n",
    "Handles [N, C, L] format correctly\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MultiChannelTimeSeriesImageConverter:\n",
    "    \"\"\"\n",
    "    Convert multivariate time series to multiple images (one per channel).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 image_height: int = 224,\n",
    "                 image_width: int = 224,\n",
    "                 viz_type: str = 'line_plot'):\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.viz_type = viz_type\n",
    "    \n",
    "    def convert_multivariate_sequence(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert multivariate sequence to multiple images.\n",
    "        \n",
    "        Args:\n",
    "            x: [C, L] tensor (C channels, L timesteps)\n",
    "        \n",
    "        Returns:\n",
    "            images: [C, 3, H, W] tensor (one image per channel)\n",
    "        \"\"\"\n",
    "        C, L = x.shape\n",
    "        images = []\n",
    "        \n",
    "        for c in range(C):\n",
    "            channel_data = x[c]  # [L]\n",
    "            img_array = self._create_line_plot_fast(channel_data.cpu().numpy())\n",
    "            # [H, W, 3] -> [3, H, W]\n",
    "            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float() / 255.0\n",
    "            images.append(img_tensor)\n",
    "        \n",
    "        return torch.stack(images)  # [C, 3, H, W]\n",
    "    \n",
    "    def convert_batch(self, sequences: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert batch of multivariate sequences.\n",
    "        \n",
    "        Args:\n",
    "            sequences: [B, C, L] tensor\n",
    "        \n",
    "        Returns:\n",
    "            images: [B, C, 3, H, W] tensor\n",
    "        \"\"\"\n",
    "        B, C, L = sequences.shape\n",
    "        batch_images = []\n",
    "        \n",
    "        for i in range(B):\n",
    "            channel_images = self.convert_multivariate_sequence(sequences[i])\n",
    "            batch_images.append(channel_images)\n",
    "        \n",
    "        return torch.stack(batch_images)  # [B, C, 3, H, W]\n",
    "    \n",
    "    def _create_line_plot_fast(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Fast line plot (SAME as Pix2Seq training).\"\"\"\n",
    "        img = Image.new('RGB', (self.image_width, self.image_height), color='white')\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        x_min, x_max = x.min(), x.max()\n",
    "        if x_max - x_min < 1e-8:\n",
    "            x_norm = np.ones_like(x) * 0.5\n",
    "        else:\n",
    "            x_norm = (x - x_min) / (x_max - x_min)\n",
    "        \n",
    "        margin = int(self.image_height * 0.05)\n",
    "        y_coords = self.image_height - margin - (x_norm * (self.image_height - 2*margin)).astype(int)\n",
    "        x_coords = np.linspace(0, self.image_width-1, len(x)).astype(int)\n",
    "        \n",
    "        points = list(zip(x_coords.tolist(), y_coords.tolist()))\n",
    "        if len(points) > 1:\n",
    "            draw.line(points, fill='blue', width=2)\n",
    "        \n",
    "        return np.array(img)\n",
    "\n",
    "\n",
    "def convert_har_multivariate_to_images(\n",
    "    data_dir='data/HAR',\n",
    "    output_dir='data/HAR/multichannel_images',\n",
    "    image_size=224,\n",
    "    viz_type='line_plot'\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert multivariate HAR to multi-channel images.\n",
    "    Handles [N, C, L] format correctly.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"CONVERTING MULTIVARIATE HAR TO MULTI-CHANNEL IMAGES\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  - Image size: {image_size}x{image_size}\")\n",
    "    print(f\"  - Visualization: {viz_type}\")\n",
    "    print(f\"  - Output: {output_dir}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\n[1/4] Loading HAR data...\")\n",
    "    train_dict = torch.load(f'{data_dir}/train.pt')\n",
    "    val_dict = torch.load(f'{data_dir}/val.pt')\n",
    "    test_dict = torch.load(f'{data_dir}/test.pt')\n",
    "    \n",
    "    X_train = train_dict['samples']  # [N, 9, 128]\n",
    "    y_train = train_dict['labels']   # [N]\n",
    "    X_val = val_dict['samples']      # [N, 9, 128]\n",
    "    y_val = val_dict['labels']       # [N]\n",
    "    X_test = test_dict['samples']    # [N, 9, 128]\n",
    "    y_test = test_dict['labels']     # [N]\n",
    "    \n",
    "    print(f\"  Train: {X_train.shape}, labels: {y_train.shape}\")\n",
    "    print(f\"  Val: {X_val.shape}, labels: {y_val.shape}\")\n",
    "    print(f\"  Test: {X_test.shape}, labels: {y_test.shape}\")\n",
    "    \n",
    "    # Verify shapes\n",
    "    assert X_train.dim() == 3, f\"Expected 3D tensor, got {X_train.dim()}D\"\n",
    "    assert X_train.shape[1] == 9, f\"Expected 9 channels, got {X_train.shape[1]}\"\n",
    "    assert X_train.shape[2] == 128, f\"Expected 128 timesteps, got {X_train.shape[2]}\"\n",
    "    \n",
    "    # Initialize converter\n",
    "    print(\"\\n[2/4] Initializing multi-channel converter...\")\n",
    "    converter = MultiChannelTimeSeriesImageConverter(\n",
    "        image_height=image_size,\n",
    "        image_width=image_size,\n",
    "        viz_type=viz_type\n",
    "    )\n",
    "    print(\"  ✓ Converter ready\")\n",
    "    \n",
    "    # Convert\n",
    "    print(\"\\n[3/4] Converting sequences to multi-channel images...\")\n",
    "    batch_size = 50\n",
    "    \n",
    "    # Train\n",
    "    print(\"  Converting train...\")\n",
    "    train_images = []\n",
    "    for i in tqdm(range(0, len(X_train), batch_size), desc=\"Train\"):\n",
    "        batch = X_train[i:i+batch_size]\n",
    "        batch_images = converter.convert_batch(batch)\n",
    "        train_images.append(batch_images)\n",
    "    train_images = torch.cat(train_images, dim=0)\n",
    "    print(f\"    ✓ Train images: {train_images.shape}\")  # [N, 9, 3, 224, 224]\n",
    "    \n",
    "    # Val\n",
    "    print(\"  Converting val...\")\n",
    "    val_images = []\n",
    "    for i in tqdm(range(0, len(X_val), batch_size), desc=\"Val\"):\n",
    "        batch = X_val[i:i+batch_size]\n",
    "        batch_images = converter.convert_batch(batch)\n",
    "        val_images.append(batch_images)\n",
    "    val_images = torch.cat(val_images, dim=0)\n",
    "    print(f\"    ✓ Val images: {val_images.shape}\")\n",
    "    \n",
    "    # Test\n",
    "    print(\"  Converting test...\")\n",
    "    test_images = []\n",
    "    for i in tqdm(range(0, len(X_test), batch_size), desc=\"Test\"):\n",
    "        batch = X_test[i:i+batch_size]\n",
    "        batch_images = converter.convert_batch(batch)\n",
    "        test_images.append(batch_images)\n",
    "    test_images = torch.cat(test_images, dim=0)\n",
    "    print(f\"    ✓ Test images: {test_images.shape}\")\n",
    "    \n",
    "    # Verify all conversions match labels\n",
    "    assert len(train_images) == len(y_train), \"Train size mismatch!\"\n",
    "    assert len(val_images) == len(y_val), \"Val size mismatch!\"\n",
    "    assert len(test_images) == len(y_test), \"Test size mismatch!\"\n",
    "    \n",
    "    # Save\n",
    "    print(\"\\n[4/4] Saving datasets...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    torch.save({\n",
    "        'images': train_images,      # [N, 9, 3, 224, 224]\n",
    "        'labels': y_train,           # [N]\n",
    "        'sequences': X_train,        # [N, 9, 128]\n",
    "        'num_channels': 9,\n",
    "        'sequence_length': 128,\n",
    "        'image_size': image_size\n",
    "    }, f'{output_dir}/train_multichannel_images.pt')\n",
    "    \n",
    "    torch.save({\n",
    "        'images': val_images,\n",
    "        'labels': y_val,\n",
    "        'sequences': X_val,\n",
    "        'num_channels': 9,\n",
    "        'sequence_length': 128,\n",
    "        'image_size': image_size\n",
    "    }, f'{output_dir}/val_multichannel_images.pt')\n",
    "    \n",
    "    torch.save({\n",
    "        'images': test_images,\n",
    "        'labels': y_test,\n",
    "        'sequences': X_test,\n",
    "        'num_channels': 9,\n",
    "        'sequence_length': 128,\n",
    "        'image_size': image_size\n",
    "    }, f'{output_dir}/test_multichannel_images.pt')\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'num_classes': 6,\n",
    "        'class_names': {\n",
    "            0: 'WALKING',\n",
    "            1: 'WALKING_UPSTAIRS',\n",
    "            2: 'WALKING_DOWNSTAIRS',\n",
    "            3: 'SITTING',\n",
    "            4: 'STANDING',\n",
    "            5: 'LAYING'\n",
    "        },\n",
    "        'num_channels': 9,\n",
    "        'channel_names': [\n",
    "            'body_acc_x', 'body_acc_y', 'body_acc_z',\n",
    "            'body_gyro_x', 'body_gyro_y', 'body_gyro_z',\n",
    "            'total_acc_x', 'total_acc_y', 'total_acc_z'\n",
    "        ],\n",
    "        'sequence_length': 128,\n",
    "        'image_size': image_size,\n",
    "        'viz_type': viz_type,\n",
    "        'train_samples': len(train_images),\n",
    "        'val_samples': len(val_images),\n",
    "        'test_samples': len(test_images)\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(f'{output_dir}/metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    # Save sample visualizations\n",
    "    print(\"\\nSaving sample visualizations...\")\n",
    "    sample_dir = f'{output_dir}/samples'\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "    \n",
    "    # Save first 5 samples, showing all 9 channels\n",
    "    for i in range(min(5, len(train_images))):\n",
    "        sample_img = train_images[i]  # [9, 3, 224, 224]\n",
    "        label = y_train[i].item()\n",
    "        class_name = metadata['class_names'][label]\n",
    "        \n",
    "        # Create grid of 9 channel images (3x3)\n",
    "        import torchvision.utils as vutils\n",
    "        grid = vutils.make_grid(sample_img, nrow=3, padding=2)\n",
    "        grid_np = grid.permute(1, 2, 0).numpy() * 255\n",
    "        grid_np = grid_np.astype(np.uint8)\n",
    "        \n",
    "        Image.fromarray(grid_np).save(\n",
    "            f'{sample_dir}/sample_{i}_label_{label}_{class_name}_all_channels.png'\n",
    "        )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✓ CONVERSION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nFiles created:\")\n",
    "    print(f\"  - {output_dir}/train_multichannel_images.pt\")\n",
    "    print(f\"      Images: {train_images.shape}\")\n",
    "    print(f\"      Labels: {y_train.shape}\")\n",
    "    print(f\"  - {output_dir}/val_multichannel_images.pt\")\n",
    "    print(f\"      Images: {val_images.shape}\")\n",
    "    print(f\"      Labels: {y_val.shape}\")\n",
    "    print(f\"  - {output_dir}/test_multichannel_images.pt\")\n",
    "    print(f\"      Images: {test_images.shape}\")\n",
    "    print(f\"      Labels: {y_test.shape}\")\n",
    "    print(f\"  - {output_dir}/metadata.json\")\n",
    "    print(f\"  - {sample_dir}/*.png (sample visualizations)\")\n",
    "    print(f\"\\n✅ Each sequence converted to 9 separate images (one per channel)\")\n",
    "    print(f\"✅ Images use SAME format as Pix2Seq training\")\n",
    "    \n",
    "    return train_images, val_images, test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334f8fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_har_multivariate_to_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d0d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# TRAINING SCRIPT\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from data_loader import create_har_dataloaders\n",
    "from models.encoder_cls import EncoderClassifier\n",
    "\n",
    "# Enable these at the top of your script\n",
    "torch.backends.cudnn.benchmark = True  # Auto-tune conv operations\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Faster matmul on Ampere+ GPUs\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "\n",
    "# Use mixed precision training\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "def train_har_classifier(\n",
    "    mode='flatten',\n",
    "    batch_size=32,\n",
    "    num_epochs=20,\n",
    "    learning_rate=1e-3,\n",
    "    device='cuda',\n",
    "    save_dir='checkpoints/har_classifier'\n",
    "):\n",
    "    \"\"\"\n",
    "    Train HAR classifier with pretrained Pix2Seq encoder.\n",
    "    \n",
    "    Args:\n",
    "        mode: 'flatten', 'average', or 'first'\n",
    "        batch_size: Training batch size\n",
    "        num_epochs: Number of epochs\n",
    "        learning_rate: Learning rate\n",
    "        device: 'cuda' or 'cpu'\n",
    "        save_dir: Directory to save checkpoints\n",
    "    \"\"\"\n",
    "    \n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"TRAINING HAR CLASSIFIER WITH PIX2SEQ ENCODER\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  Mode: {mode}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Epochs: {num_epochs}\")\n",
    "    print(f\"  Learning rate: {learning_rate}\")\n",
    "    print(f\"  Device: {device}\")\n",
    "    \n",
    "    # Create dataloaders\n",
    "    print(\"\\n[1/5] Creating dataloaders...\")\n",
    "    train_loader, val_loader, test_loader = create_har_dataloaders(\n",
    "        mode=mode,\n",
    "        batch_size=batch_size,\n",
    "        use_augmentation=True\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\n[2/5] Loading model with pretrained encoder...\")\n",
    "    model = EncoderClassifier(\n",
    "        num_classes=6,\n",
    "        pretrained_encoder_path='/home/AD/sachith/pix2seq/data/HAR_pretrained/extracted_encoder/encoder_weights.pth',\n",
    "        freeze_encoder=True,\n",
    "        hidden_dims=[512],\n",
    "        dropout=0.1,\n",
    "        image_size=224\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    # Compile model (PyTorch 2.0+)\n",
    "    if hasattr(torch, 'compile'):\n",
    "        model = torch.compile(model, mode='reduce-overhead')\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    # Mixed precision scaler\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Training loop\n",
    "    print(\"\\n[3/5] Training...\")\n",
    "    best_val_acc = 0.0\n",
    "    train_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            optimizer.zero_grad()\n",
    "            # Mixed precision forward pass\n",
    "            with autocast('cuda'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Scaled backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Stats\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100.*train_correct/train_total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
    "        print(f'  Val Acc: {val_acc:.2f}%')\n",
    "        print(f'  LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'mode': mode\n",
    "            }, f'{save_dir}/best_model.pth')\n",
    "            print(f'  ✓ Saved best model (val_acc: {val_acc:.2f}%)')\n",
    "    \n",
    "    # Test evaluation\n",
    "    print(\"\\n[4/5] Evaluating on test set...\")\n",
    "    checkpoint = torch.load(f'{save_dir}/best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc='Testing'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    test_acc = 100. * test_correct / test_total\n",
    "    \n",
    "    # Save training history\n",
    "    print(\"\\n[5/5] Saving results...\")\n",
    "    results = {\n",
    "        'mode': mode,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'train_losses': train_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'num_epochs': num_epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate\n",
    "    }\n",
    "    \n",
    "    with open(f'{save_dir}/training_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Best Val Accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"Model saved to: {save_dir}/best_model.pth\")\n",
    "    \n",
    "    return model, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de4dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# ============================================================================\n",
    "# RUN TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "print(f\"Using GPU: {torch.cuda.current_device()}\")\n",
    "print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")  # Will show GPU 1 as device 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Training with FLATTEN mode (recommended)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING WITH FLATTEN MODE (9x more samples)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    model_flatten, results_flatten = train_har_classifier(\n",
    "        mode='flatten',\n",
    "        batch_size=128,\n",
    "        num_epochs=20,\n",
    "        learning_rate=1e-3,\n",
    "        save_dir='checkpoints/har_flatten'\n",
    "    )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
