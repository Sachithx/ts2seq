{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493bac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from hierarchical_event_labeling import EnhancedHierarchicalEventDataset\n",
    "\n",
    "# Load HAR data\n",
    "train_data = torch.load('data/HAR/train.pt')\n",
    "val_data = torch.load('data/HAR/val.pt')\n",
    "test_data = torch.load('data/HAR/test.pt')\n",
    "X_train = train_data['samples']  # [N, 9, 128] # N roughly 59,000\n",
    "X_val = val_data['samples']      # [N, 9, 128] # N roughly 14,000\n",
    "X_test = test_data['samples']     # [N, 9, 128] # N roughly 14,000\n",
    "\n",
    "# Reshape: treat each channel as separate univariate sequence\n",
    "X_train_univariate = X_train.reshape(-1, 128).float()\n",
    "X_val_univariate = X_val.reshape(-1, 128).float()\n",
    "\n",
    "\n",
    "# Your data\n",
    "train_ann_dataset = EnhancedHierarchicalEventDataset(X_train_univariate)\n",
    "print(f\"Dataset size: {len(train_ann_dataset)} samples\")\n",
    "torch.save(train_ann_dataset, 'data/HAR/har_ann_train_dataset.pt')\n",
    "\n",
    "val_ann_dataset = EnhancedHierarchicalEventDataset(X_val_univariate)\n",
    "print(f\"Dataset size: {len(val_ann_dataset)} samples\")\n",
    "torch.save(val_ann_dataset, 'data/HAR/har_ann_val_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1038ecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_checkpoint. py\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "checkpoint_path = '/projects/pix2seqdata/tmp/ts_model'\n",
    "\n",
    "# Get latest checkpoint\n",
    "ckpt = tf. train.latest_checkpoint(checkpoint_path)\n",
    "print(f\"Latest checkpoint: {ckpt}\")\n",
    "\n",
    "if ckpt:\n",
    "    # Load checkpoint\n",
    "    reader = tf.train.load_checkpoint(ckpt)\n",
    "    \n",
    "    # List all variables\n",
    "    var_names = reader.get_variable_to_shape_map()\n",
    "    \n",
    "    print(f\"\\nCheckpoint contains {len(var_names)} variables\")\n",
    "    print(\"\\nSample encoder variables:\")\n",
    "    for name in sorted(var_names. keys())[:20]: \n",
    "        print(f\"  {name}: {var_names[name]}\")\n",
    "    \n",
    "    # Check critical architecture indicators\n",
    "    encoder_vars = [k for k in var_names.keys() if 'encoder' in k.lower()]\n",
    "    print(f\"\\nEncoder variables: {len(encoder_vars)}\")\n",
    "    \n",
    "    # Try to infer architecture\n",
    "    print(\"\\nArchitecture hints:\")\n",
    "    \n",
    "    # Check number of encoder layers\n",
    "    layer_vars = [k for k in encoder_vars if 'layer_' in k or 'block_' in k]\n",
    "    if layer_vars:\n",
    "        # Extract layer numbers\n",
    "        import re\n",
    "        layer_nums = set()\n",
    "        for var in layer_vars:\n",
    "            match = re.search(r'layer_(\\d+)|block_(\\d+)', var)\n",
    "            if match:\n",
    "                num = match.group(1) or match.group(2)\n",
    "                layer_nums.add(int(num))\n",
    "        print(f\"  Detected encoder layers: {sorted(layer_nums)}\")\n",
    "        print(f\"  → num_encoder_layers = {len(layer_nums)}\")\n",
    "    \n",
    "    # Check attention dimensions\n",
    "    attn_vars = [k for k in var_names.keys() if 'attention' in k.lower() and 'kernel' in k]\n",
    "    if attn_vars:\n",
    "        sample_var = attn_vars[0]\n",
    "        shape = var_names[sample_var]\n",
    "        print(f\"  Sample attention shape: {shape}\")\n",
    "        print(f\"  → dim_att might be {shape[-1] if len(shape) > 0 else 'unknown'}\")\n",
    "\n",
    "else:\n",
    "    print(\"No checkpoint found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_config.py\n",
    "import sys\n",
    "sys.path.insert(0, '/home/yourusername/ts2seq')\n",
    "\n",
    "from configs import config_det_finetune\n",
    "\n",
    "# Load config\n",
    "config = config_det_finetune.get_config()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CURRENT CONFIG (before CLI overrides)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nModel architecture:\")\n",
    "print(f\"  name: {config.model.name}\")\n",
    "print(f\"  image_size: {config.model. get('image_size', 'NOT SET')}\")\n",
    "\n",
    "# Check all model params\n",
    "model_keys = [\n",
    "    'num_encoder_layers', 'num_decoder_layers',\n",
    "    'dim_att', 'dim_att_dec', \n",
    "    'dim_mlp', 'dim_mlp_dec',\n",
    "    'num_heads', 'num_heads_dec',\n",
    "    'encoder_variant', 'patch_size',\n",
    "]\n",
    "\n",
    "for key in model_keys: \n",
    "    value = config.model.get(key, 'NOT SET')\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2520073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_val_loss.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from configs import config_ts_224\n",
    "from data import coco as coco_data\n",
    "from models import model as model_lib\n",
    "from tasks import task as task_lib\n",
    "\n",
    "def compute_validation_loss(model_dir, config):\n",
    "    \"\"\"Compute validation loss for a trained model.\"\"\"\n",
    "    \n",
    "    # Load model and data\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    with strategy.scope():\n",
    "        # Create trainer\n",
    "        trainer = model_lib.TrainerRegistry.lookup(config.model.name)(\n",
    "            config, model_dir=model_dir\n",
    "        )\n",
    "        \n",
    "        # Load validation data\n",
    "        val_dataset = coco_data.CocoObjectDetectionTFRecordDataset(config)\n",
    "        val_dataset = val_dataset.load_dataset(training=False)\n",
    "        val_dataset = val_dataset.batch(config.eval.batch_size)\n",
    "        \n",
    "        # Create task\n",
    "        task = task_lib. TaskRegistry.lookup(config.task. name)(config)\n",
    "        \n",
    "        # Compute loss on validation set\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in val_dataset. take(100):  # Sample 100 batches\n",
    "            # Preprocess\n",
    "            preprocessed = task.preprocess_batched(batch, training=False)\n",
    "            \n",
    "            # For detection:  (images, input_seq, target_seq, token_weights)\n",
    "            if len(preprocessed) == 4:\n",
    "                images, input_seq, target_seq, token_weights = preprocessed\n",
    "                \n",
    "                # Forward pass\n",
    "                logits = trainer.model(images, input_seq, training=False)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss_per_token = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=target_seq,\n",
    "                    logits=logits\n",
    "                )\n",
    "                loss = tf.reduce_sum(loss_per_token * token_weights) / tf.reduce_sum(token_weights)\n",
    "                \n",
    "                total_loss += loss.numpy()\n",
    "                num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "        return avg_loss\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    config = config_ts_224.get_config()\n",
    "    model_dir = \"/projects/pix2seqdata/tmp/ts_model\"\n",
    "    \n",
    "    val_loss = compute_validation_loss(model_dir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DIAGNOSE ANNOTATION ISSUES\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def diagnose_annotations(annotation_file):\n",
    "    \"\"\"Analyze annotation statistics.\"\"\"\n",
    "    \n",
    "    with open(annotation_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ANNOTATION DIAGNOSTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Basic stats\n",
    "    num_images = len(data['images'])\n",
    "    num_annotations = len(data['annotations'])\n",
    "    num_categories = len(data['categories'])\n",
    "    \n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Images: {num_images}\")\n",
    "    print(f\"  Annotations:  {num_annotations}\")\n",
    "    print(f\"  Categories: {num_categories}\")\n",
    "    print(f\"  Avg annotations per image: {num_annotations/num_images:.2f}\")\n",
    "    \n",
    "    # Extract annotation data\n",
    "    widths = []\n",
    "    heights = []\n",
    "    areas = []\n",
    "    category_counts = Counter()\n",
    "    annotations_per_image = Counter()\n",
    "    \n",
    "    for ann in data['annotations']:\n",
    "        bbox = ann['bbox']  # [start, end] for time series\n",
    "        # width = bbox[1] - bbox[0]\n",
    "        width = bbox[2]  # bbox = [x, y, width, height]\n",
    "        widths.append(width)\n",
    "        \n",
    "        category_counts[ann['category_id']] += 1\n",
    "        annotations_per_image[ann['image_id']] += 1\n",
    "    \n",
    "    widths = np.array(widths)\n",
    "    \n",
    "    # Width statistics\n",
    "    print(f\"\\nBox Width Statistics:\")\n",
    "    print(f\"  Min: {widths.min()}\")\n",
    "    print(f\"  Max: {widths.max()}\")\n",
    "    print(f\"  Mean: {widths.mean():.2f}\")\n",
    "    print(f\"  Median: {np.median(widths):.2f}\")\n",
    "    print(f\"  Std: {widths. std():.2f}\")\n",
    "    \n",
    "    # Width distribution\n",
    "    print(f\"\\n  Width percentiles:\")\n",
    "    for p in [10, 25, 50, 75, 90, 95, 99]: \n",
    "        val = np.percentile(widths, p)\n",
    "        print(f\"    {p}th:  {val:.1f}\")\n",
    "    \n",
    "    # Zero-width boxes\n",
    "    zero_width = (widths == 0).sum()\n",
    "    if zero_width > 0:\n",
    "        print(f\"\\n  ⚠️ WARNING: {zero_width} ({zero_width/len(widths)*100:.1f}%) boxes have ZERO width!\")\n",
    "    \n",
    "    # Small boxes (COCO definition:  area < 32^2 = 1024, for 1D: width < 32)\n",
    "    small_boxes = (widths < 32).sum()\n",
    "    medium_boxes = ((widths >= 32) & (widths < 96)).sum()\n",
    "    large_boxes = (widths >= 96).sum()\n",
    "    \n",
    "    print(f\"\\n  Box size distribution (COCO thresholds):\")\n",
    "    print(f\"    Small (width < 32): {small_boxes} ({small_boxes/len(widths)*100:.1f}%)\")\n",
    "    print(f\"    Medium (32 ≤ width < 96): {medium_boxes} ({medium_boxes/len(widths)*100:.1f}%)\")\n",
    "    print(f\"    Large (width ≥ 96): {large_boxes} ({large_boxes/len(widths)*100:.1f}%)\")\n",
    "    \n",
    "    # Category distribution\n",
    "    print(f\"\\nCategory Distribution:\")\n",
    "    for cat_id, count in category_counts.most_common():\n",
    "        cat_name = next((c['name'] for c in data['categories'] if c['id'] == cat_id), 'Unknown')\n",
    "        print(f\"  {cat_name} (ID {cat_id}): {count} ({count/num_annotations*100:.1f}%)\")\n",
    "    \n",
    "    # Annotations per image\n",
    "    ann_counts = list(annotations_per_image.values())\n",
    "    print(f\"\\nAnnotations per Image:\")\n",
    "    print(f\"  Min: {min(ann_counts)}\")\n",
    "    print(f\"  Max: {max(ann_counts)}\")\n",
    "    print(f\"  Mean:  {np.mean(ann_counts):.2f}\")\n",
    "    print(f\"  Median: {np.median(ann_counts):.2f}\")\n",
    "    \n",
    "    # Plot distributions\n",
    "    fig, axes = plt. subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Width distribution\n",
    "    axes[0, 0].hist(widths, bins=50, edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('Box Width')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].set_title('Box Width Distribution')\n",
    "    axes[0, 0].axvline(32, color='r', linestyle='--', label='Small/Medium threshold')\n",
    "    axes[0, 0].axvline(96, color='orange', linestyle='--', label='Medium/Large threshold')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Category distribution\n",
    "    cat_names = [c['name'][:15] for c in sorted(data['categories'], key=lambda x: x['id'])]\n",
    "    cat_counts = [category_counts[c['id']] for c in sorted(data['categories'], key=lambda x: x['id'])]\n",
    "    axes[0, 1].bar(range(len(cat_names)), cat_counts)\n",
    "    axes[0, 1].set_xticks(range(len(cat_names)))\n",
    "    axes[0, 1].set_xticklabels(cat_names, rotation=45, ha='right')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].set_title('Category Distribution')\n",
    "    \n",
    "    # Annotations per image\n",
    "    axes[1, 0].hist(ann_counts, bins=30, edgecolor='black')\n",
    "    axes[1, 0]. set_xlabel('Annotations per Image')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    axes[1, 0].set_title('Annotations per Image Distribution')\n",
    "    \n",
    "    # Cumulative width distribution\n",
    "    sorted_widths = np.sort(widths)\n",
    "    cumulative = np.arange(1, len(sorted_widths) + 1) / len(sorted_widths)\n",
    "    axes[1, 1]. plot(sorted_widths, cumulative)\n",
    "    axes[1, 1]. set_xlabel('Box Width')\n",
    "    axes[1, 1]. set_ylabel('Cumulative Proportion')\n",
    "    axes[1, 1].set_title('Cumulative Width Distribution')\n",
    "    axes[1, 1].axvline(32, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[1, 1].axvline(96, color='orange', linestyle='--', alpha=0.5)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('annotation_diagnostics.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\n✓ Saved diagnostics plot to annotation_diagnostics.png\")\n",
    "    \n",
    "    return {\n",
    "        'num_annotations': num_annotations,\n",
    "        'num_images': num_images,\n",
    "        'avg_per_image': num_annotations / num_images,\n",
    "        'zero_width':  zero_width,\n",
    "        'small_ratio': small_boxes / len(widths),\n",
    "        'category_counts': category_counts\n",
    "    }\n",
    "\n",
    "# Run diagnostics\n",
    "stats = diagnose_annotations('/projects/pix2seqdata/tmp/ts_coco/annotations/instances_val.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
